{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Modeling\n",
    "\n",
    "Author: Shray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    \"\"\"Load flat file to pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_data (df1, df2, key = None, left_index=False, right_index=False):\n",
    "    \"\"\"perform inner join to return data which is present in both dataframes\"\"\"\n",
    "    return pd.merge(left=df1, right=df2, how='inner', on=key, left_index=left_index, right_index=right_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_df):\n",
    "    \"\"\"remove data which are outliers or duplicates\"\"\"\n",
    "    \n",
    "    clean_df = raw_df.drop_duplicates(subset ='jobId')\n",
    "    clean_df = clean_df[clean_df.salary>0]\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_feature_df(df, cat_vars = None, num_vars = None):\n",
    "    \"\"\"perform one hot encoding an all categorical varaibles and merge with continous variables\"\"\"\n",
    "    \n",
    "    cat_df = pd.get_dummies(df[cat_vars])\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    \n",
    "    return pd.concat([cat_df, num_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(df, target):\n",
    "    \"\"\"return target dataframe\"\"\"\n",
    "    \n",
    "    return df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std):\n",
    "    \n",
    "    neg_mse = cross_val_score(model, feature_df, target_df, cv = 2, n_jobs = num_procs, scoring= 'neg_mean_squared_error')\n",
    "    mean_mse[model] = -1.0 * np.mean(neg_mse)\n",
    "    cv_std[model] = np.std(neg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(model, mean_mse, cv_std):\n",
    "    \n",
    "    print ('\\nModel: \\n', model)\n",
    "    print ('Average MSE: \\n', mean_mse[model])\n",
    "    print ('Standard Deviation during CV: \\n', cv_std[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model, mean_mse, predictions, feature_importances):\n",
    "    \"\"\"save model, model summary, feature importances, and prediction\"\"\"\n",
    "    \n",
    "    with open ('model.txt', 'w') as file:\n",
    "        file.write(str(model))\n",
    "    feature_importances.to_csv('feature_importance.csv')\n",
    "    np.savetxt('predictions.csv', predictions, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading_data\n",
      "Data being encoded\n"
     ]
    }
   ],
   "source": [
    "#define inputs\n",
    "\n",
    "train_feature_file = 'train_features.csv'\n",
    "train_target_file = 'train_salaries.csv'\n",
    "test_feature_file = 'test_features.csv'\n",
    "\n",
    "# define variables\n",
    "\n",
    "cat_vars = ['companyId', 'jobType', 'degree', 'major', 'industry']\n",
    "num_vars = ['yearsExperience', 'milesFromMetropolis']\n",
    "target_vars = 'salary'\n",
    "\n",
    "# Load data\n",
    "\n",
    "print (\"Loading_data\")\n",
    "feature_df = load_file(train_feature_file)\n",
    "target_df = load_file(train_target_file)\n",
    "test_df = load_file(test_feature_file)\n",
    "\n",
    "# consolidate training data\n",
    "\n",
    "raw_train_df =  consolidate_data(feature_df, target_df, key = 'jobId')\n",
    "\n",
    "# clean the data\n",
    "\n",
    "clean_train_df = shuffle(clean_data(raw_train_df)).reset_index()\n",
    "\n",
    "# encoding and getting final features\n",
    "\n",
    "print(\"Data being encoded\")\n",
    "feature_df = one_hot_encode_feature_df(clean_train_df, cat_vars = cat_vars, num_vars = num_vars)\n",
    "test_df = one_hot_encode_feature_df(test_df, cat_vars = cat_vars, num_vars = num_vars)\n",
    "\n",
    "# get target df\n",
    "\n",
    "target_df = get_target(clean_train_df, target_vars)\n",
    "\n",
    "# starting model list and dicts\n",
    "\n",
    "models = []\n",
    "mean_mse = {}\n",
    "cv_std = {}\n",
    "res = {}\n",
    "\n",
    "# number of parallel process\n",
    "\n",
    "num_procs = 2\n",
    "\n",
    "#shared model parameters\n",
    "\n",
    "verbose_lvl = 5\n",
    "\n",
    "# create models\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr_std_pca = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 60, n_jobs = num_procs, max_depth = 25, min_samples_split = 60,\\\n",
    "                          max_features = 30, verbose = verbose_lvl)\n",
    "\n",
    "gbm = GradientBoostingRegressor(n_estimators = 40, max_depth = 5, loss = 'ls', verbose = verbose_lvl)\n",
    "\n",
    "models.extend([lr, lr_std_pca, rf, gbm])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Started\n",
      "\n",
      "Model: \n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False)\n",
      "Average MSE: \n",
      " 384.44894477823595\n",
      "Standard Deviation during CV: \n",
      " 0.312732345519521\n",
      "\n",
      "Model: \n",
      " Pipeline(memory=None,\n",
      "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False))])\n",
      "Average MSE: \n",
      " 384.4466837523429\n",
      "Standard Deviation during CV: \n",
      " 0.3192933893919019\n",
      "\n",
      "Model: \n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
      "           max_features=30, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=60, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=60, n_jobs=2, oob_score=False, random_state=None,\n",
      "           verbose=5, warm_start=False)\n",
      "Average MSE: \n",
      " 368.72888224044664\n",
      "Standard Deviation during CV: \n",
      " 0.35420734294072537\n",
      "\n",
      "Model: \n",
      " GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=5, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=40, n_iter_no_change=None, presort='auto',\n",
      "             random_state=None, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=5, warm_start=False)\n",
      "Average MSE: \n",
      " 398.564850181293\n",
      "Standard Deviation during CV: \n",
      " 0.6200991130232865\n",
      "\n",
      "Predictions calculated with lowest MSE model:\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
      "           max_features=30, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=60, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=60, n_jobs=2, oob_score=False, random_state=None,\n",
      "           verbose=5, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Cross validating models and MSE taken as evaluation metrics\n",
    "\n",
    "print (\"Cross Validation Started\")\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std)\n",
    "    print_summary(model, mean_mse, cv_std)\n",
    "    \n",
    "# Selecting model with lowest MSE\n",
    "\n",
    "model = min(mean_mse, key = mean_mse.get)\n",
    "print ('\\nPredictions calculated with lowest MSE model:')\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 60\n",
      "building tree 2 of 60\n",
      "building tree 3 of 60\n",
      "building tree 4 of 60\n",
      "building tree 5 of 60\n",
      "building tree 6 of 60\n",
      "building tree 7 of 60\n",
      "building tree 8 of 60\n",
      "building tree 9 of 60\n",
      "building tree 10 of 60\n",
      "building tree 11 of 60\n",
      "building tree 12 of 60\n",
      "building tree 13 of 60\n",
      "building tree 14 of 60\n",
      "building tree 15 of 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:  1.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 16 of 60\n",
      "building tree 17 of 60\n",
      "building tree 18 of 60\n",
      "building tree 19 of 60\n",
      "building tree 20 of 60\n",
      "building tree 21 of 60\n",
      "building tree 22 of 60\n",
      "building tree 23 of 60\n",
      "building tree 24 of 60\n",
      "building tree 25 of 60\n",
      "building tree 26 of 60\n",
      "building tree 27 of 60\n",
      "building tree 28 of 60\n",
      "building tree 29 of 60\n",
      "building tree 30 of 60\n",
      "building tree 31 of 60\n",
      "building tree 32 of 60\n",
      "building tree 33 of 60\n",
      "building tree 34 of 60\n",
      "building tree 35 of 60\n",
      "building tree 36 of 60\n",
      "building tree 37 of 60\n",
      "building tree 38 of 60\n",
      "building tree 39 of 60\n",
      "building tree 40 of 60\n",
      "building tree 41 of 60\n",
      "building tree 42 of 60\n",
      "building tree 43 of 60\n",
      "building tree 44 of 60\n",
      "building tree 45 of 60\n",
      "building tree 46 of 60\n",
      "building tree 47 of 60\n",
      "building tree 48 of 60\n",
      "building tree 49 of 60\n",
      "building tree 50 of 60\n",
      "building tree 51 of 60\n",
      "building tree 52 of 60\n",
      "building tree 53 of 60\n",
      "building tree 54 of 60\n",
      "building tree 55 of 60\n",
      "building tree 56 of 60\n",
      "building tree 57 of 60\n",
      "building tree 58 of 60\n",
      "building tree 59 of 60\n",
      "building tree 60 of 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:   18.7s finished\n"
     ]
    }
   ],
   "source": [
    "# training on entire daataset\n",
    "\n",
    "model.fit(feature_df, target_df)\n",
    "\n",
    "# Create predictions based on test data\n",
    "predictions = model.predict(test_df)\n",
    "\n",
    "# Store feature importances\n",
    "\n",
    "if hasattr(model, 'feature_importance_'):\n",
    "    importances = model.feature_importances_\n",
    "else:\n",
    "    importances = [0]*len(feature_df.columns)\n",
    "    \n",
    "feature_importances = pd.DataFrame({'feature': feature_df.columns, 'importance': importances})\n",
    "feature_importances.sort_values(by = 'importance', ascending =  False, inplace = True)\n",
    "\n",
    "#set index to feature\n",
    "\n",
    "feature_importances.set_index('feature', inplace = True, drop = True)\n",
    "\n",
    "#save results\n",
    "save_results(model, mean_mse[model], predictions, feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-794fa09a63d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_importances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_importances' is not defined"
     ]
    }
   ],
   "source": [
    "feature_importances[0:25].plot.bar(figsize = (20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
